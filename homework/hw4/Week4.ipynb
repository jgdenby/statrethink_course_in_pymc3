{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import theano\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, compute the entropy of each island’s birb distribution. Interpret these entropy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Island 1', 0.2, 0.2, 0.2, 0.2, 0.2], ['Island 2', 0.8, 0.1, 0.05, 0.025, 0.025], ['Island 3', 0.05, 0.15, 0.7, 0.05, 0.05]] \n",
    "\n",
    "df = (pd.DataFrame(data, columns = ['Island', 'Bird A', 'Bird B', 'Bird C', 'Bird D', 'Bird E'])\n",
    "      .set_index(\"Island\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = df.apply(lambda x: stats.entropy(x.values), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of uncertainty, with higher entropy meaning a more uniform distribution (and thus less surprisal at any given outcome). Island 1 has an equal distribution of all bird types (completely uniform), so its entropy is the largest of the three. Island 3 has mostly Bird A (80%), so its bird distribution has the lowest entropy. Island 2 is somewhere in between the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Second, use each island’s birb distribution to predict the other two. This means to compute the K-L Divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different K-L Divergence values. Which island predicts the others best? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df[f'KL_island{i+1}'] = df.apply(lambda x: kl_divergence(x.values, df.iloc[i].values), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence offers a way to measure the surprisal of one distribution using another as a model. As we might expect, using Island 1 as a model results in, on average, lower divergence scores for the other islands. This is because Island 1 has maximum entropy - its uniform distribution of birds means it has no strong expectations that could be violated by the life on other islands. On the other hand, Islands 2 and 3 each have relatively peaked distributions, with just a single bird variety comprising the majority of life. As such, using them as a model means we expect that bird, and are subsequently very surprised when traveling to another island where that bird is much less prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models `m6.9` and `m6.10` again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, pre-processing according to the models' specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = pd.read_csv('../../data/happiness.csv', header=0)\n",
    "happy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adults = happy[happy['age'] > 17].copy()\n",
    "adults['age'] = (adults['age'] - 18) / (65 - 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m6.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model69:\n",
    "    # data\n",
    "    happiness = adults['happiness']\n",
    "    age = adults['age']\n",
    "    married = adults['married'].values\n",
    "    \n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu = 0, sigma = 1, shape = 2) # an intercept per married category\n",
    "    beta_A = pm.Normal('beta_A', mu = 0, sigma = 2)\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha[married] + beta_A * age\n",
    "    happiness_hat = pm.Normal('happiness_hat', mu = mu, sigma = sigma, observed = happiness)\n",
    "    \n",
    "    # sampling\n",
    "    posterior69 = pm.sample(draws = 1000, tune = 1000)\n",
    "    posterior_pred69 = pm.sample_posterior_predictive(posterior69)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(posterior69, alpha = .11).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model estimates a negative association between age and happiness, even though the data generating process assures that there is no such connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model610:\n",
    "    # data\n",
    "    happiness = adults['happiness']\n",
    "    age = adults['age']\n",
    "    married = adults['married'].values\n",
    "    \n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu = 0, sigma = 1) \n",
    "    beta_A = pm.Normal('beta_A', mu = 0, sigma = 2)\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_A * age\n",
    "    happiness_hat = pm.Normal('happiness_hat', mu = mu, sigma = sigma, observed = happiness)\n",
    "    \n",
    "    # sampling\n",
    "    posterior610 = pm.sample(draws = 1000, tune = 1000)\n",
    "    posterior_pred610 = pm.sample_posterior_predictive(posterior610)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(posterior610, alpha = .11).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we omit marriage as an indicator variable, the age effect goes away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare these two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model69.name = \"model69\"\n",
    "model610.name = \"model610\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.compare({model69: posterior69, model610: posterior610})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m6.9` has the lower WAIC value, even though it's confounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reconsider the urban fox analysis from last week’s homework. Use WAIC or LOO based model comparison on five different models, each using `weight` as the outcome, and containing these sets of predictor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foxes = pd.read_csv('../../data/foxes.csv', sep=';', header=0)\n",
    "foxes[['avgfood','groupsize','area','weight']] = preprocessing.scale(foxes[['avgfood','groupsize','area','weight']])\n",
    "foxes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be using the same data for all our models, we can set the feature vectors up at the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgfood = theano.shared(np.array(foxes.avgfood))\n",
    "groupsize = theano.shared(np.array(foxes.groupsize))\n",
    "area = theano.shared(np.array(foxes.area))\n",
    "weight = theano.shared(np.array(foxes.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model1:\n",
    "    #priors\n",
    "    alpha = pm.Normal('alpha', 0, .5)\n",
    "    beta_food = pm.Normal('beta_food', 0, .5)\n",
    "    beta_groupsize = pm.Normal('beta_groupsize', 0, .5)\n",
    "    beta_area = pm.Normal('beta_area', 0, .5)\n",
    "    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_food * avgfood + beta_groupsize * groupsize + beta_area* area\n",
    "    weight_hat = pm.Normal('weight_hat', mu = mu, sigma = sigma, observed = weight)\n",
    "    \n",
    "    #sampling\n",
    "    posterior1 = pm.sample(draws = 1000, tune = 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    #priors\n",
    "    alpha = pm.Normal('alpha', 0, .5)\n",
    "    beta_food = pm.Normal('beta_food', 0, .5)\n",
    "    beta_groupsize = pm.Normal('beta_groupsize', 0, .5)\n",
    "    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_food * avgfood + beta_groupsize * groupsize \n",
    "    weight_hat = pm.Normal('weight_hat', mu = mu, sigma = sigma, observed = weight)\n",
    "    \n",
    "    #sampling\n",
    "    posterior2 = pm.sample(draws = 1000, tune = 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model3:\n",
    "    #priors\n",
    "    alpha = pm.Normal('alpha', 0, .5)\n",
    "    beta_area = pm.Normal('beta_area', 0, .5)\n",
    "    beta_groupsize = pm.Normal('beta_groupsize', 0, .5)\n",
    "    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_area * area + beta_groupsize * groupsize \n",
    "    weight_hat = pm.Normal('weight_hat', mu = mu, sigma = sigma, observed = weight)\n",
    "    \n",
    "    #sampling\n",
    "    posterior3 = pm.sample(draws = 1000, tune = 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model4:\n",
    "    #priors\n",
    "    alpha = pm.Normal('alpha', 0, .5)\n",
    "    beta_food = pm.Normal('beta_food', 0, .5)    \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_food * avgfood\n",
    "    weight_hat = pm.Normal('weight_hat', mu = mu, sigma = sigma, observed = weight)\n",
    "    \n",
    "    #sampling\n",
    "    posterior4 = pm.sample(draws = 1000, tune = 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model5:\n",
    "    #priors\n",
    "    alpha = pm.Normal('alpha', 0, .5)\n",
    "    beta_area = pm.Normal('beta_area', 0, .5)\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # model\n",
    "    mu = alpha + beta_area * area\n",
    "    weight_hat = pm.Normal('weight_hat', mu = mu, sigma = sigma, observed = weight)\n",
    "    \n",
    "    #sampling\n",
    "    posterior5 = pm.sample(draws = 1000, tune = 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.name = 'model1'\n",
    "model2.name = 'model2'\n",
    "model3.name = 'model3'\n",
    "model4.name = 'model4'\n",
    "model5.name = 'model5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.compare({\n",
    "    model1: posterior1,\n",
    "    model2: posterior2,\n",
    "    model3: posterior3,\n",
    "    model4: posterior4,\n",
    "    model5: posterior5\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
